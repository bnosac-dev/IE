{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create vocabularies of words and tags from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from collections import Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra word for PAD (padding) and UNK (unrecognized word)\n",
    "PAD_WORD = '<PAD>'\n",
    "PAD_TAG = 'O'\n",
    "UNK_WORD = '<UNK>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I/O helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dict_to_json(d, out_path):\n",
    "    \"\"\"\n",
    "    Writes a dictionary object to a json file\n",
    "    - d (dict)\n",
    "    - output (str): path to the output filename\n",
    "    \"\"\"\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(d, f, indent=4)\n",
    "        \n",
    "def write_list_to_file(myIterable, file_path):\n",
    "    \"\"\"\n",
    "    Write each element in the input to a new line\n",
    "    Args:\n",
    "    - myIterable (iterable)\n",
    "    - file_path (str): path to the file\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        for ele in myIterable:\n",
    "            f.write( str(ele)+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sys.path)\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_utils import data_converter, conlleval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data_iob = data_converter.read_conll('../data/eng.train')[1:] #ignore header\n",
    "train_data_bio = data_converter.read_conll('../data/train.bio')[1:] #ignore header\n",
    "testa_data_bio = data_converter.read_conll('../data/testa.bio')[1:]\n",
    "testb_data = data_converter.read_conll('../data/eng.testb')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train_data_bio.copy()\n",
    "all_data.extend(testa_data_bio)\n",
    "all_data.extend(testb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes\n",
      " train, dev, test, all\n",
      "[14985, 3464, 3683, 22132]\n"
     ]
    }
   ],
   "source": [
    "datasets = [train_data_bio, testa_data_bio, testb_data, all_data]\n",
    "print(\"Dataset sizes\")\n",
    "print(\" train, testa, testb, all\")\n",
    "print(list(map(lambda data: len(data), datasets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collect all words as we encounter in the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for dataset in datasets:\n",
    "    for sent in dataset:\n",
    "        for (w, *_) in sent:\n",
    "            all_words.append(w)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token occurances:  605600\n",
      "Number of unique words:  30289\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of token occurances: \", len(all_words))\n",
    "word_counter = Counter(all_words)\n",
    "print(\"Number of unique words: \", len(word_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.hist(all_words, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Add PAD and UNK as extra words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Montoya 4\n",
      "Tacambaro 4\n",
      "convoy 4\n",
      "SANTIAGO 4\n",
      "gift 4\n",
      "Woman 4\n",
      "Sleeping 4\n",
      "Fidel 4\n",
      "novel 4\n",
      "Granma 4\n"
     ]
    }
   ],
   "source": [
    "# hyperparameter for thresholding the most common N words to keep in our vocab\n",
    "N_MOST_COMMON = 15000 #len(word_counter)/2\n",
    "common_words = word_counter.most_common(N_MOST_COMMON)\n",
    "plt.hist(common_words.values(),bins=50)\n",
    "# OR, filter by MIN_COUNT and MAX_COUNT\n",
    "# MIN_COUNT = 5\n",
    "# MAX_COUNT = \n",
    "for i,(w,c) in enumerate(common_words[::-1]):\n",
    "    if i<10:\n",
    "        print(w,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "words = [ w for w,_ in common_words]\n",
    "print(len(words) == len(common_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PAD and UNK as extra words\n",
    "words.extend([PAD_WORD, UNK_WORD])\n",
    "write_list_to_file(words, '../data/words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15002\n"
     ]
    }
   ],
   "source": [
    "word2idx = { w:i for i,w in enumerate(words) }\n",
    "print(len(word2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(word2idx, \"../data/word2idx.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag to idx mapping \n",
    "tag2idx = {}\n",
    "it = 0 #conunters\n",
    "for sent in train_data_bio:\n",
    "    for (*_, tag) in sent:\n",
    "        if tag in tag2idx:\n",
    "            continue\n",
    "        else:\n",
    "            tag2idx[tag] = it\n",
    "            it += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8, '<START>': 9, '<STOP>': 10}\n"
     ]
    }
   ],
   "source": [
    "# Add START and STOP tags\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'\n",
    "# Add START_TAG and STOP_TAG to tag2idx\n",
    "temp_n = len(tag2idx)\n",
    "tag2idx.update( {START_TAG:temp_n,\n",
    "                 STOP_TAG:temp_n+1} )\n",
    "print(tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dict_to_json(tag2idx, '../data/tag2idx.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences proprocessing\n",
    "We need to map each word in a sentence to an index in our `word2idx` map. Similarly, we need to map each tag in a tag sequence to an index in `tag2idx`. We perform this transformation on each dataset. Two subtle, yet important details are:\n",
    "\n",
    "1.  If we encounter a word that doesn't exist in our `word2idx` map (because the word was not one of the most N_MOST_COMMON words), then we mark the word as UNK word by assigning the index=`word2idx['<UNK>']`.  \n",
    "\n",
    "2. Our RNN model takes in the sequence of sentences of same length.  Therefore, we fill in a shorter sentences to SEQ_LENGTH (50 by default) using PAD_WORD. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of unknown words:  9682\n"
     ]
    }
   ],
   "source": [
    "train_sentences = []\n",
    "train_labels = []\n",
    "c_unks = 0\n",
    "for sent in train_data_bio:\n",
    "    sent_i = []\n",
    "    tag_i = []\n",
    "    for (w,*_,t) in sent:\n",
    "        idx = word2idx[w] if w in word2idx else word2idx[UNK_WORD]\n",
    "        sent_i.append(idx)\n",
    "        tag_i.append(tag2idx[t])\n",
    "        \n",
    "        # count how many unknown words in train dataset\n",
    "        if w not in word2idx:\n",
    "            c_unks += 1\n",
    "            \n",
    "    train_sentences.append(sent_i)\n",
    "print(\"Num. of unknown words: \", c_unks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word occurances:\n",
      " 204562\n"
     ]
    }
   ],
   "source": [
    "print(\"Total word occurances:\\n\",\n",
    "      len([w for sent in train_data_bio for w,*_ in sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_list_to_file(train_sentences, '../data/train_sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_labelled_dataset(dataset, word2idx, tag2idx):\n",
    "    \"\"\"\n",
    "    dataset: a list of sentence. Sentence is a list of word_infos.\n",
    "            word_info is a tuple of (word, POS, ..., tag)\n",
    "    Returns:\n",
    "    train_sentences and train_labels after mapping to indices space.\n",
    "    \"\"\"\n",
    "    train_sentences = []\n",
    "    train_labels = []\n",
    "    c_unks = 0\n",
    "    for sent in dataset:\n",
    "        sent_i = []\n",
    "        tag_i = []\n",
    "        for (w,*_,t) in sent:\n",
    "            idx = word2idx[w] if w in word2idx else word2idx[UNK_WORD]\n",
    "            sent_i.append(idx)\n",
    "            tag_i.append(tag2idx[t])\n",
    "\n",
    "            # count how many unknown words in train dataset\n",
    "            if w not in word2idx:\n",
    "                c_unks += 1\n",
    "\n",
    "        train_sentences.append(sent_i)\n",
    "        train_labels.append(tag_i)\n",
    "    print(\"Num. of unknown words: \", c_unks)\n",
    "    return train_sentences, train_labels\n",
    "\n",
    "def process_unlabelled_dataset(dataset, word2idx, tag2idx):\n",
    "    \"\"\"\n",
    "    dataset: a list of sentence. Sentence is a list of word_infos.\n",
    "            word_info is a tuple of (word, POS). Notice this does\n",
    "            not include tag (label)\n",
    "    Returns:\n",
    "    - sentences: a list of sentence after mapping each word to indices\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    c_unks = 0\n",
    "    for sent in dataset:\n",
    "        sent_i = []\n",
    "        for (w,*_) in sent:\n",
    "            idx = word2idx[w] if w in word2idx else word2idx[UNK_WORD]\n",
    "            sent_i.append(idx)\n",
    "\n",
    "            # count how many unknown words in train dataset\n",
    "            if w not in word2idx:\n",
    "                c_unks += 1\n",
    "\n",
    "        sentences.append(sent_i)\n",
    "    print(\"Num. of unknown words: \", c_unks)\n",
    "    return sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. of unknown words:  9682\n",
      "Num. of unknown words:  2547\n",
      "Num. of unknown words:  3549\n"
     ]
    }
   ],
   "source": [
    "train_sentences, train_labels = process_labelled_dataset(train_data_bio, word2idx, tag2idx)\n",
    "testa_sentences, testa_labels = process_labelled_dataset(testa_data_bio, word2idx, tag2idx)\n",
    "test_sentences = process_unlabelled_dataset(test_data, word2idx, tag2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_list_to_file(train_sentences, '../data/train_sentences.txt')\n",
    "# write_list_to_file(train_labels, '../data/train_labels.txt')\n",
    "\n",
    "# write_list_to_file(dev_sentences, '../data/dev_sentences.txt')\n",
    "# write_list_to_file(dev_labels, '../data/dev_labels.txt')\n",
    "\n",
    "# write_list_to_file(test_sentences, '../data/test_sentences.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/all_train_labels.sav']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "all_train_sentences = np.array(train_sentences)\n",
    "all_train_labels = np.array(train_labels)\n",
    "joblib.dump(all_train_sentences, '../data/all_train_sentences.sav')\n",
    "joblib.dump(all_train_labels, '../data/all_train_labels.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/test_sentences.sav']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(testa_sentences, '../data/testa_sentences.sav')\n",
    "joblib.dump(testa_labels, '../data/testa_labels.sav')\n",
    "\n",
    "joblib.dump(testb_sentences, '../data/testb_sentences.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/tag2idx.sav']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(word2idx, '../data/word2idx.sav')\n",
    "joblib.dump(tag2idx, '../data/tag2idx.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14985"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare some dev set\n",
    "len(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14985"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dev = int(len(train_sentences)*0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4495\n"
     ]
    }
   ],
   "source": [
    "print(n_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "shuffled:  [9140, 4747, 7470, 3488, 4291, 188, 2335, 7385, 10654, 1722]\n"
     ]
    }
   ],
   "source": [
    "indices = [i for i in range(len(all_train_sentences))]\n",
    "print(indices[:10])\n",
    "shuffle(indices)\n",
    "print(\"shuffled: \", indices[:10])\n",
    "\n",
    "# get indices for dev and train data\n",
    "dev_indices = indices[:n_dev]\n",
    "train_indices = indices[n_dev:]\n",
    "\n",
    "# split to train and dev data\n",
    "train_sentences = all_train_sentences[train_indices]\n",
    "train_labels = all_train_labels[train_indices]\n",
    "\n",
    "dev_sentences = all_train_sentences[dev_indices]\n",
    "dev_labels = all_train_labels[dev_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10490"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4495"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/dev_labels.sav']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(train_sentences, '../data/train_sentences.sav')\n",
    "joblib.dump(train_labels, '../data/train_labels.sav')\n",
    "\n",
    "joblib.dump(dev_sentences, '../data/dev_sentences.sav')\n",
    "joblib.dump(dev_labels, '../data/dev_labels.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
