{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-1. NER with LSTM + FC layer\n",
    "For Part 2, I tried two different models. The first model is a LSTM unit + fully-connected layer and the second model is a Bidirectional LSTM unit with CRF. The second model learns the word embeddings through bidirectional LSTM and use the output word vectors as input sequences to the CRF based model to predict the probability over the tag sequences and the weights on features.  This notebook focuses on the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/hayley/miniconda3/envs/fastai/lib/python36.zip', '/home/hayley/miniconda3/envs/fastai/lib/python3.6', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/lib-dynload', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/defusedxml-0.5.0-py3.6.egg', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/IPython/extensions', '/home/hayley/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import random\n",
    "print(sys.path)\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "# sklearn imports\n",
    "import sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "# from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict\n",
    "\n",
    "# sklearn_crfsuite imports\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "# pytorch imports \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# train logging\n",
    "import logging\n",
    "from tqdm import trange\n",
    "# import .utils as my_utils\n",
    "from nlp_utils.model_evaluate import evaluate\n",
    "from nlp_utils import model_utils\n",
    "# set a random seed\n",
    "torch.manual_seed(10);\n",
    "\n",
    "# model saving and inspection\n",
    "import joblib\n",
    "import eli5\n",
    "\n",
    "import pdb \n",
    "\n",
    "# auto-reloads\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn version: 0.20.0\n",
      "pytorch version: 0.4.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"sklearn version: {sklearn.__version__}\")\n",
    "print(f\"pytorch version: {torch.__version__}\")\n",
    "# make sure we are using pytorch > 0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/home/hayley/miniconda3/envs/fastai/lib/python36.zip', '/home/hayley/miniconda3/envs/fastai/lib/python3.6', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/lib-dynload', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/defusedxml-0.5.0-py3.6.egg', '/home/hayley/miniconda3/envs/fastai/lib/python3.6/site-packages/IPython/extensions', '/home/hayley/.ipython', '..']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra word for PAD (padding) and UNK (unrecognized word)\n",
    "PAD_WORD = '<PAD>'\n",
    "PAD_TAG = 'O'\n",
    "UNK_WORD = '<UNK>'\n",
    "\n",
    "START_TAG = '<START>'\n",
    "STOP_TAG = '<STOP>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the CRF models, we don't need to hand-engineer the features for our word representation. We use the index mapper (from word to index and tag to index) as we defined in `create_vocab.ipynb` and in the Step section above. To summarize, we use the most common $N$ words out of the total words that occured in all the datasets provided (`eng:train`, `eng:testa`, `eng:testb`). In our experiments $N$ is set $15,000$ which is half of the number of words that occured at least once in any of the given datasets. I added several extra words: PAD_WORD and UNK_WORD. PAD_WORDs are appended to sentences in a mini-batch to make every sentence of equal length (as to the length of the longest sentence in the mini-batch). UNK_WORD is used to map words that are not in our vocab (because we excluded 15,000 uncommon words). In addition, I assigned a PAD_TAG to indicate the padding words, and START_TAG and STOP_TAG. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed sentences and labels as well as the word2idx and tag2idx dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = joblib.load('../data/train_sentences.sav')\n",
    "train_labels = joblib.load('../data/train_labels.sav')\n",
    "\n",
    "dev_sentences = joblib.load('../data/testa_sentences.sav')\n",
    "dev_labels = joblib.load('../data/testa_labels.sav')\n",
    "\n",
    "test_sentences = joblib.load('../data/testb_sentences.sav')\n",
    "\n",
    "# indice mappers\n",
    "word2idx = joblib.load('../data/word2idx.sav')\n",
    "tag2idx = joblib.load('../data/tag2idx.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sentences:  10490 10490\n",
      "dev sentences:  3464 3464\n",
      "test sentences:  3683\n",
      "vocab size:  15002\n",
      "number of tags:  11\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics on the datasets and lookup tables\n",
    "print('train sentences: ', len(train_sentences), len(train_labels))\n",
    "print('dev sentences: ', len(dev_sentences), len(dev_labels))\n",
    "print('test sentences: ', len(test_sentences))\n",
    "print('vocab size: ', len(word2idx))\n",
    "print('number of tags: ', len(tag2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data loaders for mini-batch of sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we sample a batch of sentences, the sentences usually have a different length. In a batch of sentences, (`batch_sentences`) with correspoonding batch of tags `batch_tags`, we add PAD_WORD for sentences that have fewer words than SQE_LENGTH (set to maximum length of a sentence in the current `batch_sentences`). Below shows this processing in code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3929c630a0fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This is just to show the processing and is not meant to actually run.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Maximum sentence lengths in current batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatch_max_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_sentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Intial matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "# This is just to show the processing and is not meant to actually run.\n",
    "# Maximum sentence lengths in current batch \n",
    "batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "# Intial matrix\n",
    "batch_data = word2idx[PAD_WORD]*np.ones((len(batch_sentences), batch_max_len))\n",
    "batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "\n",
    "# Fill in the matrix with current batch sentences and labels\n",
    "for i in range(len(batch_sentences)):\n",
    "    curr_len = len(batch_sentences[i])\n",
    "    batch_data[i][:curr_len] = batch_sentences[i]\n",
    "    batch_labels[i][:curr_len] = batch_tags[i]\n",
    "\n",
    "# Convert to torch.LongTensors (since each entry is an index)\n",
    "batch_data = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of the processing above, our `batch_data` now consists of the same length of sequences,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceDataLoader(object):\n",
    "    \"\"\"\n",
    "    Loads a mini-batch of data at each iterations. \n",
    "    Stores the following properties:\n",
    "    - dataset_params\n",
    "    - word2idx: word to index mapping\n",
    "    - tag2idx: tag to index mapping\n",
    "    \n",
    "    Args:\n",
    "    - data_dir (str): path to the directory containing the dataset\n",
    "    - parsm (dict): hyperparameters for data loading\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, word2idx_file, tag2idx_file):\n",
    "        self.data_dir = data_dir\n",
    "        \n",
    "        word2idx_path = os.path.join(data_dir, word2idx_file)\n",
    "        tag2idx_path = os.path.join(data_dir, tag2idx_file)\n",
    "        self.word2idx = joblib.load(word2idx_path)\n",
    "        self.tag2idx = joblib.load(tag2idx_path)\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "        self.n_tags = len(self.tag2idx)\n",
    "        \n",
    "    def load_sentences_labels(self, sentences_path, labels_path, d):\n",
    "        \"\"\"\n",
    "        Load sentences and labels for this dataset to the input dictionary d\n",
    "        \"\"\"\n",
    "        d['data'] = joblib.load(sentences_path)\n",
    "        d['labels'] = joblib.load(labels_path)\n",
    "        d['size'] = len(d['data'])\n",
    "        \n",
    "        \n",
    "    def load_data(self,types):\n",
    "        \"\"\"\n",
    "        Load dataset(s) from data_dir.\n",
    "        Args:\n",
    "        - data_dir (str): path to the directory that contains dataset files\n",
    "        - types (list): a list of string(s) which is one of 'train', 'dev', 'test'\n",
    "        \n",
    "        Returns:\n",
    "        - data (dict): contains the sentences and labels for each type in types\n",
    "        \"\"\"            \n",
    "        data = {}\n",
    "        for split in ['train', 'dev', 'testa']:\n",
    "            if split in types:\n",
    "                sentences_path = os.path.join(self.data_dir, split+\"_sentences.sav\")\n",
    "                labels_path = os.path.join(self.data_dir, split+\"_labels.sav\")\n",
    "                print(sentences_path, \"\\n\", labels_path)\n",
    "                                     \n",
    "                data[split] = {}\n",
    "                self.load_sentences_labels(sentences_path, labels_path, data[split])\n",
    "        return data\n",
    "    \n",
    "    def data_iterator(self, data, params, shuffle=False):\n",
    "        \"\"\"\n",
    "        Returns a generator that yields a mini-batch of data (sentences and labels).\n",
    "        It iteratates once over the data\n",
    "        \n",
    "        Args:\n",
    "        - data (dict): a dictionary with keys of 'data', 'labels', 'size'\n",
    "        - params (dict): hyperparams of the training \n",
    "        - shuffle (bool): to shuffle the mini-batch or not\n",
    "        \n",
    "        Yields:\n",
    "        - batch_data (torch.LongTensor): word indices of size batch_size * seq_len \n",
    "        - batch_labels (torch.LongTensor): tag indices of size batch_size * seq_len\n",
    "        \"\"\"\n",
    "        order = list(range(data['size']))\n",
    "        if shuffle:\n",
    "            random.seed(0)\n",
    "            random.shuffle(order)\n",
    "        \n",
    "        # One iteration over data\n",
    "        for i in range( (data['size']+1)//params.batch_size ):\n",
    "            # Get a batch of sentences and tags \n",
    "            batch_sentences = [data['data'][i] for i in order[i*params.batch_size: (i+1)*params.batch_size]]\n",
    "            batch_tags = [data['labels'][i] for i in order[i*params.batch_size: (i+1)*params.batch_size]]\n",
    "            \n",
    "            # Perform the two modification mentioned above\n",
    "            # Append PAD words so that all sentences are of the same length in each batch\n",
    "            # mark unseen word's tag as -1\n",
    "            # Maximum sentence lengths in current batch \n",
    "            batch_max_len = max([len(s) for s in batch_sentences])\n",
    "\n",
    "            # Intial matrix\n",
    "            ## Use -1 for initial tags to differentiate it with tags from PAD_WORDs\n",
    "            batch_data = self.word2idx[PAD_WORD]*np.ones((len(batch_sentences), batch_max_len))\n",
    "            batch_labels = -1*np.ones((len(batch_sentences), batch_max_len))\n",
    "#             print(type(batch_data), type(batch_labels))\n",
    "\n",
    "\n",
    "            # Fill in the matrix with current batch sentences and labels\n",
    "            for i in range(len(batch_sentences)):\n",
    "                curr_len = len(batch_sentences[i])\n",
    "                batch_data[i][:curr_len] = batch_sentences[i]\n",
    "                batch_labels[i][:curr_len] = batch_tags[i]\n",
    "\n",
    "            # Convert to torch.LongTensors (since each entry is an index)\n",
    "#             batch_data = torch.LongTensor(batch_data), torch.LongTensor(batch_labels)\n",
    "            batch_data, batch_labels = torch.from_numpy(batch_data), torch.from_numpy(batch_labels)\n",
    "            batch_data = batch_data.type(torch.LongTensor)\n",
    "            batch_labels = batch_labels.type(torch.LongTensor)\n",
    "#             print(type(batch_data), type(batch_labels))\n",
    "\n",
    "            # If gpu available\n",
    "            if params.cuda:\n",
    "                batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a data_iterator function using this logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data contains train_sentences and tarin_labels\n",
    "# params is a dictionary that contains a key of 'batch_size'\n",
    "loader = SentenceDataLoader('../data', 'word2idx.sav', 'tag2idx.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15002"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.vocab_size\n",
    "# train_iterator = data_iterator(train_data, dataiter_params, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train_sentences.sav \n",
      " ../data/train_labels.sav\n",
      "../data/dev_sentences.sav \n",
      " ../data/dev_labels.sav\n",
      "../data/testa_sentences.sav \n",
      " ../data/testa_labels.sav\n"
     ]
    }
   ],
   "source": [
    "d = loader.load_data(['train', 'dev', 'testa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = d['train']\n",
    "dev_data = d['dev']\n",
    "test_data = d['testa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model_utils.Params('../data/base_params.json')\n",
    "\n",
    "diter=loader.data_iterator(train_data, params)\n",
    "bdata, blabels = next(diter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "bdata=bdata.type(torch.LongTensor);print(bdata.type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: RNN (LSTM + FC)\n",
    "This model is largely taken from [cs230](https://github.com/cs230-stanford/cs230-code-examples/blob/master/pytorch/nlp/model/net.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        RNN model (lstm) that predicts the NER tags for each token in the sentence. It is composed of:\n",
    "        \n",
    "        - an embedding layer: maps each index in range(params.vocab_size) to a params.embedding_dim vector\n",
    "        - lstm: applying the LSTM on the sequential input returns an output for each token in the sentence\n",
    "        - fc: a fully connected layer that converts the LSTM output for each token to a distribution over NER tags\n",
    "        \n",
    "        Args:\n",
    "            params (dict): contains vocab_size, embedding_dim, lstm_hidden_dim\n",
    "        \"\"\"\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(params.vocab_size, params.embedding_dim)\n",
    "        self.lstm = nn.LSTM(params.embedding_dim, params.lstm_hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(params.lstm_hidden_dim, params.number_of_tags)\n",
    "        \n",
    "    def forward(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (torch.tensor): a batch of sentences, of dimension batch_size x seq_len, where seq_len is\n",
    "               the length of the longest sentence in the batch. For sentences shorter than seq_len, the remaining\n",
    "               tokens are PAD_WORD. Each row is a sentence with each element corresponding to the index of\n",
    "               the token in the vocab.\n",
    "        Returns:\n",
    "            out (torch.tensor): of dimension batch_size*seq_len x num_tags with the log probabilities of tokens for each token\n",
    "                 of each sentence.\n",
    "        Note: the dimensions after each step are provided\n",
    "        \"\"\"\n",
    "        #                                -> batch_size x seq_len\n",
    "        # apply the embedding layer that maps each token to its embedding\n",
    "        s = self.embed(s)            # dim: batch_size x seq_len x embedding_dim\n",
    "\n",
    "        # run the LSTM along the sentences of length seq_len\n",
    "        s, _ = self.lstm(s)              # dim: batch_size x seq_len x lstm_hidden_dim\n",
    "\n",
    "        # make the Variable contiguous in memory (a PyTorch artefact)\n",
    "        s = s.contiguous()\n",
    "\n",
    "        # reshape the Variable so that each row contains one token\n",
    "        s = s.view(-1, s.shape[2])       # dim: batch_size*seq_len x lstm_hidden_dim\n",
    "\n",
    "        # apply the fully connected layer and obtain the output (before softmax) for each token\n",
    "        s = self.fc(s)                   # dim: batch_size*seq_len x num_tags\n",
    "\n",
    "        # apply log softmax on each token's output (this is recommended over applying softmax\n",
    "        # since it is numerically more stable)\n",
    "        return F.log_softmax(s, dim=1)   # dim: batch_size*seq_len x num_tags\n",
    "\n",
    "\n",
    "def loss_fn(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the cross entropy loss given outputs from the model and labels for all tokens. Exclude loss terms\n",
    "    for PADding tokens.\n",
    "    Args:\n",
    "        outputs: (Variable) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (Variable) dimension batch_size x seq_len where each element is either a label in [0, 1, ... num_tag-1],\n",
    "                or -1 in case it is a PADding token.\n",
    "    Returns:\n",
    "        loss: (Variable) cross entropy loss for all tokens in the batch\n",
    "    Note: you may use a standard loss function from http://pytorch.org/docs/master/nn.html#loss-functions. This example\n",
    "          demonstrates how you can easily define a custom loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.view(-1)\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0).float()\n",
    "\n",
    "    # indexing with negative values is not supported. Since PADded tokens have label -1, we convert them to a positive\n",
    "    # number. This does not affect training, since we ignore the PADded tokens with the mask.\n",
    "    labels = labels % outputs.shape[1]\n",
    "\n",
    "    num_tokens = int(torch.sum(mask).data[0])\n",
    "\n",
    "    # compute cross entropy loss for all tokens (except PADding tokens), by multiplying with mask.\n",
    "    return -torch.sum(outputs[range(outputs.shape[0]), labels]*mask)/num_tokens\n",
    "\n",
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Compute the accuracy, given the outputs and labels for all tokens. Exclude PADding terms.\n",
    "    Args:\n",
    "        outputs: (np.ndarray) dimension batch_size*seq_len x num_tags - log softmax output of the model\n",
    "        labels: (np.ndarray) dimension batch_size x seq_len where each element is either a label in\n",
    "                [0, 1, ... num_tag-1], or -1 in case it is a PADding token.\n",
    "    Returns: (float) accuracy in [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape labels to give a flat vector of length batch_size*seq_len\n",
    "    labels = labels.ravel()\n",
    "\n",
    "    # since PADding tokens have label -1, we can generate a mask to exclude the loss from those terms\n",
    "    mask = (labels >= 0)\n",
    "\n",
    "    # np.argmax gives us the class predicted for each token by the model\n",
    "    outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "    # compare outputs with labels and divide by number of tokens (excluding PADding tokens)\n",
    "    return np.sum(outputs==labels)/float(np.sum(mask))\n",
    "\n",
    "\n",
    "# maintain all metrics required in this dictionary- these are used in the training and evaluation loops\n",
    "metrics = {\n",
    "    'accuracy': accuracy,\n",
    "    # could add more metrics such as accuracy for each token type\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training \n",
    "def train(model, optimizer, loss_fn, data_iterator, metrics, params, num_steps):\n",
    "    \"\"\"Train the model on `num_steps` batches\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        data_iterator: (generator) a generator that generates batches of data and labels\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        num_steps: (int) number of batches to train on, each of size params.batch_size\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # summary for current training loop and a running average object for loss\n",
    "    summ = []\n",
    "    loss_avg = model_utils.RunningAverage()\n",
    "    \n",
    "    # Use tqdm for progress bar\n",
    "    t = trange(num_steps) \n",
    "    for i in t:\n",
    "        # fetch the next training batch\n",
    "\n",
    "        train_batch, labels_batch = next(data_iterator)        \n",
    "\n",
    "        # compute model output and loss\n",
    "        output_batch = model(train_batch)\n",
    "        loss = loss_fn(output_batch, labels_batch)\n",
    "\n",
    "        # clear previous gradients, compute gradients of all variables wrt loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # performs updates using calculated gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluate summaries only once in a while\n",
    "        if i % params.save_summary_steps == 0:\n",
    "            # extract data from torch Variable, move to cpu, convert to numpy arrays\n",
    "            output_batch = output_batch.data.cpu().numpy()\n",
    "            labels_batch = labels_batch.data.cpu().numpy()\n",
    "\n",
    "            # compute all metrics on this batch\n",
    "            summary_batch = {metric:metrics[metric](output_batch, labels_batch)\n",
    "                             for metric in metrics}\n",
    "            summary_batch['loss'] = loss.data[0]\n",
    "            summ.append(summary_batch)\n",
    "\n",
    "        # update the average loss\n",
    "        loss_avg.update(loss.data[0])\n",
    "        t.set_postfix(loss='{:05.3f}'.format(loss_avg()))\n",
    "\n",
    "    # compute mean of all metrics in summary\n",
    "    metrics_mean = {metric:np.mean([x[metric] for x in summ]) for metric in summ[0]} \n",
    "    metrics_string = \" ; \".join(\"{}: {:05.3f}\".format(k, v) for k, v in metrics_mean.items())\n",
    "    logging.info(\"- Train metrics: \" + metrics_string)\n",
    "    \n",
    "\n",
    "def train_and_evaluate(model, train_data, val_data, optimizer, loss_fn, metrics, params, model_dir, restore_file=None):\n",
    "    \"\"\"Train the model and evaluate every epoch.\n",
    "    Args:\n",
    "        model: (torch.nn.Module) the neural network\n",
    "        train_data: (dict) training data with keys 'data' and 'labels'\n",
    "        val_data: (dict) validaion data with keys 'data' and 'labels'\n",
    "        optimizer: (torch.optim) optimizer for parameters of model\n",
    "        loss_fn: a function that takes batch_output and batch_labels and computes the loss for the batch\n",
    "        metrics: (dict) a dictionary of functions that compute a metric using the output and labels of each batch\n",
    "        params: (Params) hyperparameters\n",
    "        model_dir: (string) directory containing config, weights and log\n",
    "        restore_file: (string) optional- name of file to restore from (without its extension .pth.tar)\n",
    "    \"\"\"\n",
    "    # reload weights from restore_file if specified\n",
    "#     if restore_file is not None:\n",
    "#         restore_path = os.path.join(model_dir, restore_file + '.pth.tar')\n",
    "#         logging.info(\"Restoring parameters from {}\".format(restore_path))\n",
    "#         nlp_utils.load_checkpoint(restore_path, model, optimizer)\n",
    "        \n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    for epoch in range(params.num_epochs):\n",
    "        # Run one epoch\n",
    "        logging.info(\"Epoch {}/{}\".format(epoch + 1, params.num_epochs))\n",
    "\n",
    "        # compute number of batches in one epoch (one full pass over the training set)\n",
    "        num_steps = (params.train_size + 1) // params.batch_size\n",
    "        train_data_iterator = data_loader.data_iterator(train_data, params, shuffle=True)\n",
    "        train(model, optimizer, loss_fn, train_data_iterator, metrics, params, num_steps)\n",
    "            \n",
    "        # Evaluate for one epoch on validation set\n",
    "        num_steps = (params.val_size + 1) // params.batch_size\n",
    "        val_data_iterator = data_loader.data_iterator(val_data, params, shuffle=False)\n",
    "        val_metrics = evaluate(model, loss_fn, val_data_iterator, metrics, params, num_steps)\n",
    "        \n",
    "        val_acc = val_metrics['accuracy']\n",
    "        is_best = val_acc >= best_val_acc\n",
    "\n",
    "        # Save weights\n",
    "        model_utils.save_checkpoint({'epoch': epoch + 1,\n",
    "                               'state_dict': model.state_dict(),\n",
    "                               'optim_dict' : optimizer.state_dict()}, \n",
    "                               is_best=is_best,\n",
    "                               checkpoint=model_dir)\n",
    "            \n",
    "        # If best_eval, best_save_path        \n",
    "        if is_best:\n",
    "            logging.info(\"- Found new best accuracy\")\n",
    "            best_val_acc = val_acc\n",
    "            \n",
    "            # Save best val metrics in a json file in the model directory\n",
    "            best_json_path = os.path.join(model_dir, \"metrics_val_best_weights.json\")\n",
    "            model_utils.save_dict_to_json(val_metrics, best_json_path)\n",
    "\n",
    "        # Save latest val metrics in a json file in the model directory\n",
    "        last_json_path = os.path.join(model_dir, \"metrics_val_last_weights.json\")\n",
    "        model_utils.save_dict_to_json(val_metrics, last_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on `testb`\n",
    "Evaluate the model on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model_dir = '../log/progress_rnn1_10_09_12_12/'\n",
    "# data_dir = '../data'\n",
    "params = model_utils.Params('../data/rnn_1_params_2.json')\n",
    "test_model = Net(params)\n",
    "checkpoint = model_utils.load_checkpoint(os.path.join(model_dir,'best.pth.tar'), test_model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embed): Embedding(15002, 500)\n",
       "  (lstm): LSTM(500, 200, batch_first=True)\n",
       "  (fc): Linear(in_features=200, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testb dataset\n",
    "testb_sentences = joblib.load('../data/testb_sentences.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse dictionary\n",
    "idx2tag = {i:t for t,i in tag2idx.items()}\n",
    "idx2word = {i:w for w,i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "3683\n"
     ]
    }
   ],
   "source": [
    "#predict\n",
    "test_preds = []\n",
    "with torch.no_grad():\n",
    "    for i in range(len(testb_sentences)):\n",
    "        prob = test_model(torch.LongTensor([test_sentences[i]])).numpy()\n",
    "        tags_i = np.argmax(prob, axis=1)\n",
    "        tags = [idx2tag[idx] for idx in tags_i]\n",
    "        test_preds.append(tags)\n",
    "        \n",
    "        # debugging\n",
    "#         print(\"=\"*50)\n",
    "#         print(i, \"\\n\", prob.shape)\n",
    "#         print(\"number of words: \", len(tags))\n",
    "#         print(len(tags), \"\\n\", tags)\n",
    "#         words = [idx2word[j] for j in test_sentences[i]]\n",
    "#         print(words)\n",
    "#         pdb.set_trace()\n",
    "\n",
    "print(\"done\")\n",
    "print(len(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write predictions to a new column in `testb` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert the tagging scheme from BIO to IBO\n",
    "from nlp_utils import data_converter\n",
    "\n",
    "test_preds_ibo = data_converter.tags_to_conll(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the predicted labels for all words\n",
    "testb_data = data_converter.read_conll('../data/eng.testb')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check:  3683\n"
     ]
    }
   ],
   "source": [
    "augmented = data_converter.add_column(testb_data, test_preds_ibo)\n",
    "print(\"Sanity check: \", len(augmented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = data_converter.conll_to_data_stream(augmented, write_to_file='./testb_rnn1_run2_preds.txt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
